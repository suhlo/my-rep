{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "313ab865-1bf7-4ed7-9f70-4329d46499e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be3e2857-7090-4805-a180-7805af59b0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Text Language  word_count  character_count  word_density  \\\n",
      "0             தமிழ்நாடு    Tamil           1                9      0.100000   \n",
      "1  செய்தி தமிழ் இது ஒரு    Tamil           4               17      0.222222   \n",
      "2                நன்றி!    Tamil           1                6      0.142857   \n",
      "3              வணக்கம்!    Tamil           1                8      0.111111   \n",
      "4           மொழி தமிழ்?    Tamil           2               10      0.181818   \n",
      "\n",
      "   punc_count  num_vowels  vowel_density  num_exclamation_marks  \\\n",
      "0           0           0            0.0                      0   \n",
      "1           0           0            0.0                      0   \n",
      "2           1           0            0.0                      1   \n",
      "3           1           0            0.0                      1   \n",
      "4           1           0            0.0                      0   \n",
      "\n",
      "   num_question_marks  num_punctuation  num_unique_words  num_repeated_words  \\\n",
      "0                   0                0                 1                   0   \n",
      "1                   0                0                 4                   0   \n",
      "2                   0                1                 1                   0   \n",
      "3                   0                1                 1                   0   \n",
      "4                   1                1                 2                   0   \n",
      "\n",
      "   words_vs_unique  \n",
      "0              1.0  \n",
      "1              1.0  \n",
      "2              1.0  \n",
      "3              1.0  \n",
      "4              1.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "# File path to the CSV file\n",
    "file_path = \"C:/Users/Suhas sattigeri/Desktop/Mini P/data/dataset1.csv\"\n",
    "\n",
    "# Load the dataset with utf-8 encoding\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# Remove null values for the \"Text\" column\n",
    "df.dropna(subset=['Text'], inplace=True)\n",
    "\n",
    "# Convert the column \"Text\" to string type\n",
    "df['Text'] = df['Text'].astype(str)\n",
    "\n",
    "# Convert the column \"Language\" to string type\n",
    "df['Language'] = df['Language'].astype(str)\n",
    "\n",
    "# Define punctuation and vowels\n",
    "punc = ('.', ',', '!', '?', ';', ':', '-', '(', ')', '[', ']', '{', '}', \"'\", '\"')\n",
    "vowels = 'AEIOUaeiou'\n",
    "\n",
    "# Feature engineering\n",
    "df['word_count'] = df['Text'].apply(lambda x: len(x.split()))\n",
    "df['character_count'] = df['Text'].apply(lambda x: len(x.replace(\" \", \"\")))\n",
    "df['word_density'] = df['word_count'] / (df['character_count'] + 1)\n",
    "df['punc_count'] = df['Text'].apply(lambda x: len([a for a in x if a in punc]))\n",
    "df['num_vowels'] = df['Text'].apply(lambda x: sum([1 for a in x if a in vowels]))\n",
    "df['vowel_density'] = df['num_vowels'] / df['word_count']\n",
    "df['num_exclamation_marks'] = df['Text'].apply(lambda x: x.count('!'))\n",
    "df['num_question_marks'] = df['Text'].apply(lambda x: x.count('?'))\n",
    "df['num_punctuation'] = df['Text'].apply(lambda x: sum(x.count(w) for w in punc))\n",
    "df['num_unique_words'] = df['Text'].apply(lambda x: len(set(w for w in x.split())))\n",
    "df['num_repeated_words'] = df['Text'].apply(lambda x: len([w for w in collections.Counter(x.split()).values() if w > 1]))\n",
    "df['words_vs_unique'] = df['num_unique_words'] / df['word_count']\n",
    "\n",
    "# Display the first few rows of the dataframe to check the new features\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b04a1bb1-4606-4944-acaf-be95ef57d01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language                 Kannada      Tamil     Telugu\n",
      "word_count              2.239515   2.239594   2.280000\n",
      "character_count        11.596986  13.651262  11.455082\n",
      "word_density            0.183448   0.158395   0.191060\n",
      "punc_count              0.658585   0.652573   0.655082\n",
      "num_vowels              0.000000   0.000000   0.000000\n",
      "vowel_density           0.000000   0.000000   0.000000\n",
      "num_exclamation_marks   0.332569   0.318256   0.338361\n",
      "num_question_marks      0.326016   0.334317   0.316721\n",
      "num_punctuation         0.658585   0.652573   0.655082\n",
      "num_unique_words        2.239515   2.239594   2.280000\n",
      "num_repeated_words      0.000000   0.000000   0.000000\n",
      "words_vs_unique         1.000000   1.000000   1.000000\n",
      "                       word_count  character_count  word_density  punc_count  \\\n",
      "word_count               1.000000         0.570438      0.341059    0.005647   \n",
      "character_count          0.570438         1.000000     -0.514231    0.099512   \n",
      "word_density             0.341059        -0.514231      1.000000   -0.144510   \n",
      "punc_count               0.005647         0.099512     -0.144510    1.000000   \n",
      "num_vowels                    NaN              NaN           NaN         NaN   \n",
      "vowel_density                 NaN              NaN           NaN         NaN   \n",
      "num_exclamation_marks    0.006954         0.049357     -0.068055    0.508562   \n",
      "num_question_marks      -0.001250         0.051401     -0.078275    0.503918   \n",
      "num_punctuation          0.005647         0.099512     -0.144510    1.000000   \n",
      "num_unique_words         1.000000         0.570438      0.341059    0.005647   \n",
      "num_repeated_words            NaN              NaN           NaN         NaN   \n",
      "words_vs_unique               NaN              NaN           NaN         NaN   \n",
      "\n",
      "                       num_vowels  vowel_density  num_exclamation_marks  \\\n",
      "word_count                    NaN            NaN               0.006954   \n",
      "character_count               NaN            NaN               0.049357   \n",
      "word_density                  NaN            NaN              -0.068055   \n",
      "punc_count                    NaN            NaN               0.508562   \n",
      "num_vowels                    NaN            NaN                    NaN   \n",
      "vowel_density                 NaN            NaN                    NaN   \n",
      "num_exclamation_marks         NaN            NaN               1.000000   \n",
      "num_question_marks            NaN            NaN              -0.487439   \n",
      "num_punctuation               NaN            NaN               0.508562   \n",
      "num_unique_words              NaN            NaN               0.006954   \n",
      "num_repeated_words            NaN            NaN                    NaN   \n",
      "words_vs_unique               NaN            NaN                    NaN   \n",
      "\n",
      "                       num_question_marks  num_punctuation  num_unique_words  \\\n",
      "word_count                      -0.001250         0.005647          1.000000   \n",
      "character_count                  0.051401         0.099512          0.570438   \n",
      "word_density                    -0.078275        -0.144510          0.341059   \n",
      "punc_count                       0.503918         1.000000          0.005647   \n",
      "num_vowels                            NaN              NaN               NaN   \n",
      "vowel_density                         NaN              NaN               NaN   \n",
      "num_exclamation_marks           -0.487439         0.508562          0.006954   \n",
      "num_question_marks               1.000000         0.503918         -0.001250   \n",
      "num_punctuation                  0.503918         1.000000          0.005647   \n",
      "num_unique_words                -0.001250         0.005647          1.000000   \n",
      "num_repeated_words                    NaN              NaN               NaN   \n",
      "words_vs_unique                       NaN              NaN               NaN   \n",
      "\n",
      "                       num_repeated_words  words_vs_unique  \n",
      "word_count                            NaN              NaN  \n",
      "character_count                       NaN              NaN  \n",
      "word_density                          NaN              NaN  \n",
      "punc_count                            NaN              NaN  \n",
      "num_vowels                            NaN              NaN  \n",
      "vowel_density                         NaN              NaN  \n",
      "num_exclamation_marks                 NaN              NaN  \n",
      "num_question_marks                    NaN              NaN  \n",
      "num_punctuation                       NaN              NaN  \n",
      "num_unique_words                      NaN              NaN  \n",
      "num_repeated_words                    NaN              NaN  \n",
      "words_vs_unique                       NaN              NaN  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Ensure all columns used for mean calculation are numerical\n",
    "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Group by 'language' and calculate the mean, then transpose the result\n",
    "mean_by_language = df.groupby('Language')[numeric_columns].mean().T\n",
    "\n",
    "# Display the transposed result\n",
    "print(mean_by_language)\n",
    "\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Calculate the Pearson correlation matrix for numeric columns\n",
    "correlation_matrix = numeric_df.corr(method='pearson')\n",
    "\n",
    "# Display the correlation matrix\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c2b5cd-ef11-407f-add6-74bfbf30154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49b0850f-cc8d-4ec0-a5a0-afdd85e80f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Split the dataset into features and target variable\n",
    "# Load the dataset with utf-8 encoding\n",
    "df = pd.read_csv(file_path, encoding='utf-8')\n",
    "\n",
    "# Remove null values for the \"text\" column\n",
    "df.dropna(subset=['Text'], inplace=True)\n",
    "\n",
    "# Convert the column \"text\" to string type\n",
    "df['text'] = df['Text'].astype(str)\n",
    "\n",
    "# Convert the column \"language\" to string type\n",
    "df['Language'] = df['Language'].astype(str)\n",
    "\n",
    "# Remove rows where the text is empty or only whitespace\n",
    "df = df[df['Text'].str.strip() != '']\n",
    "\n",
    "# Features and labels\n",
    "X = df['Text']\n",
    "y = df['Language']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4f7b654d-2287-4b9a-aca1-dbfc7d4af82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8716548334243582\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Kannada       1.00      0.72      0.83       653\n",
      "       Tamil       1.00      0.92      0.96       605\n",
      "      Telugu       0.71      1.00      0.83       573\n",
      "\n",
      "    accuracy                           0.87      1831\n",
      "   macro avg       0.90      0.88      0.87      1831\n",
      "weighted avg       0.91      0.87      0.87      1831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and fit the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8423fe63-206b-4f5b-bab3-8276e339a432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier\n",
      "Accuracy: 0.8716548334243582\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Kannada       1.00      0.72      0.83       653\n",
      "       Tamil       1.00      0.92      0.96       605\n",
      "      Telugu       0.71      1.00      0.83       573\n",
      "\n",
      "    accuracy                           0.87      1831\n",
      "   macro avg       0.90      0.88      0.87      1831\n",
      "weighted avg       0.91      0.87      0.87      1831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a Naive Bayes classifier\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Naive Bayes Classifier\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
    "print(classification_report(y_test, y_pred_nb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40a03979-8fb3-4f99-94f4-e94a68a23ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decision Tree Classifier\n",
      "Accuracy: 0.8716548334243582\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Kannada       1.00      0.72      0.83       653\n",
      "       Tamil       1.00      0.92      0.96       605\n",
      "      Telugu       0.71      1.00      0.83       573\n",
      "\n",
      "    accuracy                           0.87      1831\n",
      "   macro avg       0.90      0.88      0.87      1831\n",
      "weighted avg       0.91      0.87      0.87      1831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Train a Decision Tree classifier\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_dt = dt_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nDecision Tree Classifier\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63574117-d503-4d51-841b-2341fea3e218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7475131c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a848c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification.py\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/Users/Suhas sattigeri/Desktop/Mini P/data/dataset2.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "df[\"Text\"] = df[\"Text\"].str.lower().str.replace(\"[^\\w\\s]\", \"\", regex=True)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X = df[\"Text\"]\n",
    "y = df[\"Language\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the vectorizer only on the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the already fitted vectorizer\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Train Decision Tree classifier\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_model(model, X_test_tfidf, y_test):\n",
    "    predictions = model.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions, average=\"weighted\")\n",
    "    recall = recall_score(y_test, predictions, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, predictions, average=\"weighted\")\n",
    "    report = classification_report(y_test, predictions)\n",
    "    return accuracy, precision, recall, f1, report\n",
    "\n",
    "nb_accuracy, nb_precision, nb_recall, nb_f1, nb_report = evaluate_model(nb_model, X_test_tfidf, y_test)\n",
    "dt_accuracy, dt_precision, dt_recall, dt_f1, dt_report = evaluate_model(dt_model, X_test_tfidf, y_test)\n",
    "\n",
    "# Export models and vectorizer for use in Flask app\n",
    "def classify_text(text):\n",
    "    text_tfidf = vectorizer.transform([text])\n",
    "    nb_prediction = nb_model.predict(text_tfidf)[0]\n",
    "    dt_prediction = dt_model.predict(text_tfidf)[0]\n",
    "    \n",
    "    nb_confidence = nb_model.predict_proba(text_tfidf)[0][\n",
    "        nb_model.classes_.tolist().index(nb_prediction)\n",
    "    ]\n",
    "    \n",
    "    dt_confidence = None\n",
    "    if hasattr(dt_model, 'predict_proba'):\n",
    "        dt_confidence = dt_model.predict_proba(text_tfidf)[0][\n",
    "            dt_model.classes_.tolist().index(dt_prediction)\n",
    "        ]\n",
    "    return {\n",
    "        \"nb_prediction\": nb_prediction,\n",
    "        \"dt_prediction\": dt_prediction,\n",
    "        \"nb_confidence\": nb_confidence,\n",
    "        \"dt_confidence\": dt_confidence\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63c8a11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug: * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suhas sattigeri\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3468: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Ensure the results are consistent by setting seed\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"C:/Users/Suhas sattigeri/Desktop/Mini P/data/cleaned_dataset1.csv\")\n",
    "df[\"Text\"] = df[\"Text\"].str.lower().str.replace(\"[^\\w\\s]\", \"\", regex=True)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X = df[\"Text\"]\n",
    "y = df[\"Language\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the vectorizer only on the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train models\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def main():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    if request.method == \"POST\":\n",
    "        text = request.get_json().get(\"text\", \"\")\n",
    "        if not text:\n",
    "            return jsonify({\"error\": \"Missing or empty text field\"}), 400\n",
    "        try:\n",
    "            language = detect(text)\n",
    "            text_tfidf = vectorizer.transform([text])\n",
    "            nb_prediction = nb_model.predict(text_tfidf)[0]\n",
    "            dt_prediction = dt_model.predict(text_tfidf)[0]\n",
    "            nb_confidence = nb_model.predict_proba(text_tfidf)[0][\n",
    "                nb_model.classes_.tolist().index(nb_prediction)\n",
    "            ]\n",
    "            dt_confidence = dt_model.predict_proba(text_tfidf)[0][\n",
    "                dt_model.classes_.tolist().index(dt_prediction)\n",
    "            ]\n",
    "            return jsonify(\n",
    "                {\n",
    "                    \"language\": language,\n",
    "                    \"nb_prediction\": nb_prediction,\n",
    "                    \"dt_prediction\": dt_prediction,\n",
    "                    \"nb_confidence\": nb_confidence * 100,\n",
    "                    \"dt_confidence\": dt_confidence * 100 if dt_confidence is not None else None,\n",
    "                }\n",
    "            )\n",
    "        except LangDetectException as e:\n",
    "            logging.error(f\"Language detection failed: {e}\")\n",
    "            return jsonify({\"error\": \"Could not detect language\"}), 500\n",
    "\n",
    "@app.route(\"/translate\", methods=[\"POST\"])\n",
    "def translate():\n",
    "    data = request.get_json()\n",
    "    text = data.get(\"text\", \"\")\n",
    "    target_language = data.get(\"target_language\", \"\")\n",
    "\n",
    "    # Replace this with actual translation logic\n",
    "    translated_text = f\"[Translated {text} to {target_language}]\"  # Placeholder translation logic\n",
    "\n",
    "    return jsonify({\"translated_text\": translated_text})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfecf738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11456f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4170db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bcf877f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on http://127.0.0.1:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug: * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suhas sattigeri\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3468: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify, render_template\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "import logging\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Ensure consistent results by setting seed\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "df = pd.read_csv(\"C:/Users/Suhas sattigeri/Desktop/Mini P/data/dataset2.csv\")\n",
    "df[\"Text\"] = df[\"Text\"].str.lower().str.replace(\"[^\\w\\s]\", \"\", regex=True)\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X = df[\"Text\"]\n",
    "y = df[\"Language\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the vectorizer on the training data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the fitted vectorizer\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Train a Decision Tree classifier\n",
    "dt_model = DecisionTreeClassifier()\n",
    "dt_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the models\n",
    "nb_predictions = nb_model.predict(X_test_tfidf)\n",
    "dt_predictions = dt_model.predict(X_test_tfidf)\n",
    "\n",
    "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
    "nb_precision = precision_score(y_test, nb_predictions, average=\"weighted\")\n",
    "nb_recall = recall_score(y_test, nb_predictions, average=\"weighted\")\n",
    "nb_f1 = f1_score(y_test, nb_predictions, average=\"weighted\")\n",
    "\n",
    "dt_accuracy = accuracy_score(y_test, dt_predictions)\n",
    "dt_precision = precision_score(y_test, dt_predictions, average=\"weighted\")\n",
    "dt_recall = recall_score(y_test, dt_predictions, average=\"weighted\")\n",
    "dt_f1 = f1_score(y_test, dt_predictions, average=\"weighted\")\n",
    "\n",
    "# Flask routes\n",
    "@app.route(\"/\")\n",
    "def main():\n",
    "    return render_template(\"main.html\")\n",
    "\n",
    "@app.route(\"/classify\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def predict():\n",
    "    if request.method == \"POST\":\n",
    "        text = request.get_json().get(\"text\", \"\")\n",
    "        if not text:\n",
    "            return jsonify({\"error\": \"Missing or empty text field\"}), 400\n",
    "        try:\n",
    "            language = detect(text)\n",
    "            text_tfidf = vectorizer.transform([text])\n",
    "            nb_prediction = nb_model.predict(text_tfidf)[0]\n",
    "            dt_prediction = dt_model.predict(text_tfidf)[0]\n",
    "            nb_confidence = nb_model.predict_proba(text_tfidf)[0][\n",
    "                nb_model.classes_.tolist().index(nb_prediction)\n",
    "            ]\n",
    "            dt_confidence = None\n",
    "            if hasattr(dt_model, 'predict_proba'):\n",
    "                dt_confidence = dt_model.predict_proba(text_tfidf)[0][\n",
    "                    dt_model.classes_.tolist().index(dt_prediction)\n",
    "                ]\n",
    "            return jsonify(\n",
    "                {\n",
    "                    \"language\": language,\n",
    "                    \"nb_prediction\": nb_prediction,\n",
    "                    \"dt_prediction\": dt_prediction,\n",
    "                    \"nb_confidence\": nb_confidence * 100,\n",
    "                    \"dt_confidence\": dt_confidence * 100 if dt_confidence is not None else None,\n",
    "                }\n",
    "            )\n",
    "        except LangDetectException as e:\n",
    "            logging.error(f\"Language detection failed: {e}\")\n",
    "            return jsonify({\"language\": \"Could not detect language\"}), 500\n",
    "\n",
    "@app.route(\"/results\")\n",
    "def results():\n",
    "    return render_template(\n",
    "        \"results.html\",\n",
    "        nb_accuracy=nb_accuracy,\n",
    "        nb_precision=nb_precision,\n",
    "        nb_recall=nb_recall,\n",
    "        nb_f1=nb_f1,\n",
    "        dt_accuracy=dt_accuracy,\n",
    "        dt_precision=dt_precision,\n",
    "        dt_recall=dt_recall,\n",
    "        dt_f1=dt_f1,\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a64f858",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
